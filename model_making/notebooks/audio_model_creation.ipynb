{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCC stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when using scc and selecting jupyter modules use these in this order\n",
    "#python3/3.10.12 tensorflow/2.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "put comments here related to library management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic functionality libraries\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# More specific tf import such as autotuner\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, GlobalAveragePooling2D, Dense, Activation, Input, Reshape, Multiply, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, ReLU, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "# Libraries for model optimizations\n",
    "import tensorflow_model_optimization as tfmot\n",
    "#from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "from keras.layers import Resizing\n",
    "from tensorflow_model_optimization.quantization.keras import quantize_annotate_layer\n",
    "import nbimporter\n",
    "import tf_keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup tensorflow GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True # set it here by hand\n",
    "\n",
    "if use_gpu:\n",
    "    # Check if GPU is available\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run .py preprocessing that sets up the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse a single serialized example from TFRecord\n",
    "def _parse_function(proto):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),  # Image is stored as a string\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),   # Label is an int64\n",
    "    }\n",
    "    \n",
    "    # Parse the input `tf.train.Example` proto using the feature description\n",
    "    parsed_example = tf.io.parse_single_example(proto, feature_description)\n",
    "    \n",
    "    # Decode the JPEG-encoded image back into a float32 tensor\n",
    "    image = tf.io.decode_jpeg(parsed_example['image'], channels=1)  # Grayscale\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # Convert to [0, 1] range\n",
    "    \n",
    "    # Reshape the image to its original shape (e.g., [124, 129, 1])\n",
    "    image = tf.reshape(image, [124, 129, 1])\n",
    "    \n",
    "    # Get the label\n",
    "    label = parsed_example['label']\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the dataset from a TFRecord file\n",
    "def load_dataset(tfrecord_file, batch_size=64):\n",
    "    # Load the TFRecord file\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "    \n",
    "    # Parse the serialized data using the _parse_function\n",
    "    parsed_dataset = raw_dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Batch and shuffle the dataset (optional, depending on your needs) buffer_size is not size of batch its size of buffer\n",
    "    parsed_dataset = parsed_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "    parsed_dataset = parsed_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train, test, and validation datasets from TFRecord files\n",
    "train_spectrogram_ds = load_dataset('train_spec_ds.tfrecord')\n",
    "test_spectrogram_ds = load_dataset('test_spec_ds.tfrecord')\n",
    "val_spectrogram_ds = load_dataset('val_spec_ds.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset size: 213\n",
      "Input shape: (64, 124, 129, 1)\n",
      "Number of classes: 6\n"
     ]
    }
   ],
   "source": [
    "new_size = len(list(train_spectrogram_ds))\n",
    "print(f\"New dataset size: {new_size}\")\n",
    "\n",
    "# Get input shape and number of classes\n",
    "for spectrograms, labels in train_spectrogram_ds.take(1):\n",
    "    input_shape = spectrograms.shape\n",
    "    num_classes = 6\n",
    "    print('Input shape:', input_shape)\n",
    "    print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset size: 27\n"
     ]
    }
   ],
   "source": [
    "new_size = len(list(test_spectrogram_ds))\n",
    "print(f\"New dataset size: {new_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Autotuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import audio_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import audio_models\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "\n",
    "no_logs = True # set to false to see printouts of models during training\n",
    "\n",
    "#class CustomLogger(logging.Logger):\n",
    "#    def __init__(self, name, level=logging.ERROR):  # Only log errors\n",
    "#        super().__init__(name, level)\n",
    "        \n",
    "# Suppress Keras Tuner logger\n",
    "logging.getLogger(\"keras_tuner\").setLevel(logging.CRITICAL)\n",
    "\n",
    "if no_logs:\n",
    "    # Suppress TensorFlow and Keras Tuner logs\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logs\n",
    "    tf.get_logger().setLevel(logging.ERROR)  # Suppress TensorFlow's Python warnings\n",
    "    logging.getLogger(\"keras_tuner\").setLevel(logging.CRITICAL)  # Suppress Keras Tuner logs\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def suppress_output():\n",
    "    \"\"\"\n",
    "    Suppress all output to stdout and stderr.\n",
    "    \"\"\"\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        sys.stdout = devnull\n",
    "        sys.stderr = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr\n",
    "\n",
    "def run_with_iteration_display(autotuner, train_data, val_data, num_epochs, max_trials):\n",
    "    \"\"\"\n",
    "    Run the tuner with suppressed output, but display the current iteration number.\n",
    "    \"\"\"\n",
    "    for current_trial in range(1, max_trials + 1):\n",
    "        # Print the current trial number outside the suppress_output context\n",
    "        print(f\"\\rTraining model {current_trial} out of {max_trials}...\", end=\"\", flush=True)\n",
    "\n",
    "        # Suppress all other outputs during the trial\n",
    "        with suppress_output():\n",
    "            try:\n",
    "                autotuner.run_tuner(train_data, val_data, num_epochs)\n",
    "            except RuntimeError as e:\n",
    "                # Handle errors gracefully without terminating the loop\n",
    "                print(f\"\\nError encountered during trial {current_trial}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    # Access and display the number of saved models\n",
    "    saved_model_count = autotuner.tuner.saved_model_count\n",
    "    print(f\"\\nTraining complete. Saved {saved_model_count} models that fit the criteria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRandomSearch(kt.RandomSearch):\n",
    "    def __init__(self, *args, save_dir=None, parameter_threshold=None, accuracy_threshold=None, oracle = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.save_dir = save_dir\n",
    "        self.parameter_threshold = parameter_threshold\n",
    "        self.accuracy_threshold = accuracy_threshold\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.saved_model_count = 0  # Counter for saved models\n",
    "\n",
    "    def run_trial(self, trial, *fit_args, **fit_kwargs):\n",
    "        current_trial = int(trial.trial_id) + 1\n",
    "        total_trials = self.oracle.max_trials\n",
    "        print(f\"\\rTraining model {current_trial} of {total_trials}\", end=\"\")  \n",
    "\n",
    "        model = self.hypermodel.build(trial.hyperparameters)\n",
    "        if hasattr(model, 'is_invalid') and model.is_invalid:\n",
    "            # For invalid models, return a fixed low score without training\n",
    "            self.oracle.update_trial(\n",
    "                trial.trial_id, {'val_accuracy': float('-inf')}\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Add early stopping callback to fit_kwargs\n",
    "        callbacks = fit_kwargs.get('callbacks', [])\n",
    "        callbacks.extend([\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                min_delta=0.01,\n",
    "                patience=3,\n",
    "                restore_best_weights=True,\n",
    "                verbose=0 #1\n",
    "            )\n",
    "        ])\n",
    "        fit_kwargs['callbacks'] = callbacks\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(*fit_args, **fit_kwargs, verbose = 0)\n",
    "\n",
    "        # Extract the best validation accuracy\n",
    "        val_accuracy = max(history.history.get('val_accuracy', [0]))\n",
    "        num_params = model.count_params()\n",
    "\n",
    "        # Save the model if thresholds are met\n",
    "        if val_accuracy > self.accuracy_threshold and num_params < self.parameter_threshold:\n",
    "            model_save_path = os.path.join(\n",
    "                self.save_dir,\n",
    "                f\"model_trial_{trial.trial_id}_acc_{int(val_accuracy * 100)}_params_{num_params}.keras\"\n",
    "            )\n",
    "            model.save(model_save_path)\n",
    "            print(f\"Saved model from trial {trial.trial_id} with accuracy: {val_accuracy:.2f} and params: {num_params}\")\n",
    "            self.saved_model_count += 1\n",
    "\n",
    "        # Report the metric to the tuner\n",
    "        self.oracle.update_trial(\n",
    "            trial.trial_id, {'val_accuracy': val_accuracy}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoTuner:\n",
    "    def __init__(self, tuner_dir, save_dir, parameter_threshold, accuracy_threshold, max_trials=10, executions_per_trial=1):\n",
    "        self.tuner_dir = tuner_dir\n",
    "        self.save_dir = save_dir\n",
    "        self.parameter_threshold = parameter_threshold\n",
    "        self.accuracy_threshold = accuracy_threshold\n",
    "        self.max_trials = max_trials\n",
    "        self.executions_per_trial = executions_per_trial\n",
    "\n",
    "        # Clean up the tuner directory if it already exists\n",
    "        if os.path.exists(self.tuner_dir):\n",
    "            shutil.rmtree(self.tuner_dir)\n",
    "            \n",
    "        # Create the oracle with a custom limit for consecutive failures\n",
    "        oracle = kt.oracles.RandomSearchOracle(\n",
    "            objective='val_accuracy',\n",
    "            max_consecutive_failed_trials=10  # Adjust as needed\n",
    "        )\n",
    "\n",
    "        # Initialize the tuner\n",
    "        self.tuner = CustomRandomSearch(\n",
    "            self.build_model,\n",
    "            oracle=oracle,\n",
    "            max_trials=self.max_trials,\n",
    "            executions_per_trial=self.executions_per_trial,\n",
    "            directory=self.tuner_dir,\n",
    "            project_name='model_tuning',\n",
    "            save_dir=self.save_dir,\n",
    "            parameter_threshold=self.parameter_threshold,\n",
    "            accuracy_threshold=self.accuracy_threshold,\n",
    "        )\n",
    "\n",
    "    def build_model(self, hp):\n",
    "        \"\"\"\n",
    "        Build model with validation checks that allow skipping invalid configurations.\n",
    "        \"\"\"\n",
    "        model = tf.keras.Sequential()\n",
    "        input_shape = (124, 129, 1)\n",
    "\n",
    "        # Input layer\n",
    "        model.add(layers.Input(shape=input_shape))\n",
    "        \n",
    "        current_height, current_width = input_shape[0], input_shape[1]\n",
    "        is_valid = True\n",
    "\n",
    "        # Convolutional layers with tunable filters, kernel size, and stride\n",
    "        for i in range(hp.Int('num_conv_layers', 1, 3)):\n",
    "            kernel_size = hp.Choice(f'kernel_size_{i}', values=[6, 8, 10, 12])\n",
    "            stride = hp.Choice(f'stride_{i}', values=[2, 4, 8, 10, 12])\n",
    "            \n",
    "            # Calculate new spatial dimensions\n",
    "            new_height = (current_height - kernel_size) // stride + 1\n",
    "            new_width = (current_width - kernel_size) // stride + 1\n",
    "            \n",
    "            # Check if dimensions are valid\n",
    "            if new_height <= 0 or new_width <= 0:\n",
    "                print(f\"Invalid configuration at layer {i}: kernel_size={kernel_size}, stride={stride}, input_shape=({current_height}, {current_width})\")\n",
    "                is_valid = False\n",
    "                break\n",
    "\n",
    "            # Add convolutional layer if valid\n",
    "            model.add(layers.Conv2D(\n",
    "                filters=hp.Int(f'filters_{i}', min_value=6, max_value=12, step=2),\n",
    "                kernel_size=kernel_size,\n",
    "                strides=stride,\n",
    "                activation='relu'\n",
    "            ))\n",
    "            model.add(layers.Dropout(rate=hp.Choice('dropout_rate', values=[0.1, 0.2, 0.3])))\n",
    "\n",
    "            # Update current dimensions\n",
    "            current_height, current_width = new_height, new_width\n",
    "\n",
    "        # If dimensions are invalid, return an invalid model\n",
    "        if not is_valid:\n",
    "            model.is_invalid = True\n",
    "            return model\n",
    "\n",
    "        # Flatten layer\n",
    "        model.add(layers.Flatten())\n",
    "\n",
    "        # Output layer\n",
    "        model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "        model_params = model.count_params()\n",
    "        if model_params > self.parameter_threshold:\n",
    "            print(f\"Model exceeds parameter limit: {model_params} params\")\n",
    "            model.is_invalid = True\n",
    "        else:\n",
    "            # Compile the model\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3])),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            model.is_invalid = False\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def run_tuner(self, train_data, val_data, num_epochs):\n",
    "        \"\"\"\n",
    "        Run the tuner to search for the best model.\n",
    "        \"\"\"\n",
    "        self.tuner.search(train_data, validation_data=val_data, epochs=num_epochs)\n",
    "        print(f\"\\nTraining complete. Saved {self.tuner.saved_model_count} models that fit the criteria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 00s]\n",
      "val_accuracy: -inf\n",
      "\n",
      "Best val_accuracy So Far: -inf\n",
      "Total elapsed time: 00h 00m 22s\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "parameter_threshold = 35000\n",
    "accuracy_threshold = 0.75\n",
    "num_epochs = 20\n",
    "max_trials = 100\n",
    "tuner_dir = 'autotuner_dir'\n",
    "save_dir = \"saved_models\"\n",
    "# Instantiate and run the AutoTuner\n",
    "autotuner = AutoTuner(tuner_dir, save_dir, parameter_threshold, accuracy_threshold, max_trials)\n",
    "run_with_iteration_display(autotuner, train_spectrogram_ds, val_spectrogram_ds, num_epochs, max_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight initializations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_index):\n",
    "    \"\"\"Plots training and validation accuracy and loss.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Model {model_index} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Model {model_index} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "histories = []\n",
    "sequential_models = True\n",
    "\n",
    "\n",
    "for i, create_model in enumerate([audio_models.create_tiny_embed_conv_model_small]):\n",
    "    print(f'\\nTraining Model {i}')\n",
    "    input_shape = (124, 129, 1)\n",
    "    model = create_model(input_shape)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    history = model.fit(\n",
    "        train_spectrogram_ds,\n",
    "        validation_data=val_spectrogram_ds,\n",
    "        epochs=10,  # You can adjust the number of epochs\n",
    "        callbacks=keras.callbacks.EarlyStopping(verbose=1, patience=2, min_delta=0.01),\n",
    "        verbose = 1\n",
    "    )\n",
    "    models.append(model)\n",
    "    histories.append(history)\n",
    "    \n",
    "    plot_training_history(history, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to reload any serialized models (from bytes) if necessary\n",
    "def load_if_serialized(model):\n",
    "    if isinstance(model, bytes):\n",
    "        # Assuming the model was saved as bytes, use `tf.keras.models.model_from_json` or similar\n",
    "        raise TypeError(\"Model is in a serialized bytes format. Please load it as a Keras model before passing.\")\n",
    "    return model\n",
    "\n",
    "def get_gzipped_model_size(model):\n",
    "    # Save the model to a temporary file in .keras format\n",
    "    with tempfile.NamedTemporaryFile(suffix='.keras', delete=False) as temp_file:\n",
    "        model.save(temp_file.name)\n",
    "        model_filename = temp_file.name\n",
    "\n",
    "    # Compress the model file to calculate its gzipped size\n",
    "    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_zip:\n",
    "        zipped_file = temp_zip.name\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(model_filename, arcname='model.keras')\n",
    "\n",
    "    # Get the size of the gzipped model file in kilobytes\n",
    "    gzipped_size = os.path.getsize(zipped_file) / 1000  # Convert bytes to KB\n",
    "\n",
    "    # Clean up temporary files\n",
    "    os.remove(model_filename)\n",
    "    os.remove(zipped_file)\n",
    "\n",
    "    return gzipped_size\n",
    "\n",
    "def plot_size_v_accuracy(models, test_spectrogram_ds):\n",
    "    # Lists to store the results\n",
    "    accuracies = []\n",
    "    model_sizes = []\n",
    "    model_names = []\n",
    "\n",
    "    # Evaluate each model and store the results\n",
    "    for i, model in enumerate(models, start=1):\n",
    "        try:\n",
    "            # Load model if it's in serialized bytes format\n",
    "            model = load_if_serialized(model)\n",
    "\n",
    "            # Check if model is a Keras model instance\n",
    "            #if not isinstance(model, tf.keras.Model):\n",
    "            #    raise TypeError(f\"Model {i} is not a Keras model instance.\")\n",
    "\n",
    "            # Check if the model is compiled by looking for an optimizer\n",
    "            if model.optimizer is None:\n",
    "                model.compile(\n",
    "                    optimizer='adam',\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy']\n",
    "                )\n",
    "            \n",
    "            # Evaluate the model\n",
    "            test_loss, test_acc = model.evaluate(test_spectrogram_ds, verbose=0)\n",
    "\n",
    "            # Calculate the gzipped model size\n",
    "            gzipped_size = get_gzipped_model_size(model)\n",
    "\n",
    "            # Print the details\n",
    "            print(f\"Model {i} ({model.name}):\")\n",
    "            print(f\"  Gzipped Model Size: {gzipped_size:.2f} KB\")\n",
    "            print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "            \n",
    "            # Store the accuracy and model size\n",
    "            accuracies.append(test_acc)\n",
    "            model_sizes.append(gzipped_size)\n",
    "            model_names.append(model.name)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with Model {i}: {e}\")\n",
    "\n",
    "    # Plotting accuracy vs. gzipped model size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(model_sizes, accuracies, marker='o', linestyle='', color='b')\n",
    "\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        plt.text(model_sizes[i], accuracies[i], model_name, fontsize=9, ha='right')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Gzipped Model Size (KB)')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title('Model Accuracy vs. Gzipped Model Size')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_size_v_accuracy(models, test_spectrogram_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shrinking the model\n",
    "\n",
    "[deployment on the edge](https://ai.google.dev/edge/litert)\n",
    "\n",
    "[general model optimizations](https://www.tensorflow.org/model_optimization/guide)\n",
    "\n",
    "We will start off with pruning\n",
    "\n",
    "we will aslo try quantization aware training to quantize the model\n",
    "and we will also try different post-training quantizations\n",
    "\n",
    "we can also try weight clustering\n",
    "\n",
    "and finalyl we will try a single workflow with a combination of the previous methods\n",
    "[collaborative optimizations](https://www.tensorflow.org/model_optimization/guide/combine/collaborative_optimization)\n",
    "\n",
    "\n",
    "https://blog.tensorflow.org/2024/04/faster-dynamically-quantized-inference-with-xnnpack.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning (pruning does not work with the generated models, ill have to fix that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions for pruning\n",
    "\n",
    "def ds_to_numpy(dataset):\n",
    "    \"\"\"Converts a tf.data.Dataset to numpy arrays for data and labels.\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for features, label in dataset:\n",
    "        data.append(features)\n",
    "        labels.append(label)\n",
    "\n",
    "    data = tf.concat(data, axis=0)\n",
    "    labels = tf.concat(labels, axis=0)\n",
    "    return data, labels\n",
    "\n",
    "def print_model_weights_sparsity(model):\n",
    "    \"\"\"Prints the sparsity (percentage of zeros) for each layer's weights in the model.\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, keras.layers.Wrapper):  # For pruned layers wrapped in the pruning wrapper\n",
    "            weights = layer.trainable_weights\n",
    "        else:\n",
    "            weights = layer.weights\n",
    "        for weight in weights:\n",
    "            weight_np = weight.numpy()\n",
    "            weight_size = weight_np.size\n",
    "            zero_num = np.count_nonzero(weight_np == 0)\n",
    "            print(f\"{weight.name}: {zero_num / weight_size:.2%} sparsity ({zero_num}/{weight_size})\")\n",
    "            \n",
    "def sequential_to_functional(sequential_model):\n",
    "    # Extract the layers from the Sequential model\n",
    "    inputs = Input(shape=sequential_model.input_shape[1:])\n",
    "    x = inputs\n",
    "\n",
    "    # Apply each layer from the Sequential model in sequence\n",
    "    for layer in sequential_model.layers:\n",
    "        x = layer(x)\n",
    "    \n",
    "    # Create a new functional model\n",
    "    functional_model = Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    # Copy weights from the Sequential model\n",
    "    functional_model.set_weights(sequential_model.get_weights())\n",
    "\n",
    "    print(functional_model)\n",
    "    \n",
    "    return functional_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, train_spectrogram_ds, test_spectrogram_ds, qat=False):\n",
    "    \"\"\"Prunes the given model with optional quantization-aware training (QAT).\"\"\"\n",
    "    # Ensure model is compatible with pruning by converting it to functional if needed\n",
    "    #model = sequential_to_functional(model)\n",
    "    \n",
    "    # Convert datasets to numpy arrays\n",
    "    train_data, train_labels = ds_to_numpy(train_spectrogram_ds)\n",
    "    test_data, test_labels = ds_to_numpy(test_spectrogram_ds)\n",
    "\n",
    "    # Define pruning parameters\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=100)\n",
    "    }\n",
    "    \n",
    "    # Define pruning callbacks\n",
    "    callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
    "\n",
    "    # Apply pruning\n",
    "    if qat:\n",
    "        # Quantization-aware training with pruning\n",
    "        quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(model)\n",
    "        pruned_model = tfmot.quantization.keras.quantize_apply(\n",
    "            quant_aware_annotate_model,\n",
    "            tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme()\n",
    "        )\n",
    "    else:\n",
    "        # Standard pruning\n",
    "        pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    # Compile the pruned model with a smaller learning rate for fine-tuning\n",
    "    pruned_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Display model summary\n",
    "    pruned_model.summary()\n",
    "\n",
    "    # Fine-tune the pruned model\n",
    "    pruned_model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        epochs=3,\n",
    "        validation_split=0.1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Strip pruning wrappers for final deployment\n",
    "    stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "    # Check sparsity levels\n",
    "    print_model_weights_sparsity(stripped_pruned_model)\n",
    "\n",
    "    # Evaluate the pruned model on test data\n",
    "    _, pruned_model_accuracy = stripped_pruned_model.evaluate(test_data, test_labels, verbose=0)\n",
    "    print(\"Pruned test accuracy:\", pruned_model_accuracy)\n",
    "\n",
    "    return stripped_pruned_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_pruned_models = []\n",
    "\n",
    "for model in models:\n",
    "    pruned_model = prune_model(model, train_spectrogram_ds, test_spectrogram_ds, qat=False)\n",
    "    basic_pruned_models.append(pruned_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_size_v_accuracy(basic_pruned_models, test_spectrogram_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning preserving quantization aware training (PQAT)\n",
    "https://www.tensorflow.org/model_optimization/guide/combine/pqat_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity and cluster preserving quantization aware training\n",
    "https://www.tensorflow.org/model_optimization/guide/combine/pcqat_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/model_optimization/guide\n",
    "\n",
    "https://blog.tensorflow.org/2024/04/faster-dynamically-quantized-inference-with-xnnpack.html\n",
    "\n",
    "\n",
    "we can also add some weight clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quantization aware training quantized models with or without pruning\n",
    "https://www.tensorflow.org/model_optimization/guide/quantization/training_example\n",
    "\n",
    "https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide#quantize_some_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_model_optimization as tfmot\n",
    "\n",
    "#quantization aware training with int8 weights and uint8 activations.\n",
    "def qat(model, pruned = False, save = False):\n",
    "  # translate model to be quantization aware\n",
    "  quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "  # Use `quantize_scope` to register the custom NoOpQuantizeConfig object\n",
    "  with tfmot.quantization.keras.quantize_scope({'NoOpQuantizeConfig': audio_models.NoOpQuantizeConfig}):\n",
    "      # Annotate the model for QAT\n",
    "      quant_aware_annotated_model = tfmot.quantization.keras.quantize_annotate_model(model)\n",
    "      \n",
    "      # Apply QAT\n",
    "      if pruned:\n",
    "        q_aware_model = tfmot.quantization.keras.quantize_apply(quant_aware_annotated_model, \n",
    "                                                                tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme())\n",
    "      else:\n",
    "        q_aware_model = tfmot.quantization.keras.quantize_apply(quant_aware_annotated_model)\n",
    "\n",
    "\n",
    "\n",
    "  # Debug: Print model type and structure\n",
    "  print(f\"Model type: {type(model)}\")\n",
    "  print(f\"Is Sequential: {isinstance(model, tf.keras.Sequential)}\")\n",
    "  print(f\"Is Functional: {hasattr(model, '_is_graph_network') and model._is_graph_network}\")\n",
    "  model.summary()\n",
    "\n",
    "  # Check if the model is Sequential or Functional\n",
    "  if not isinstance(model, tf.keras.Sequential) and not (hasattr(model, '_is_graph_network') and model._is_graph_network):\n",
    "    raise ValueError('`model` must be a Keras Sequential or Functional model.')\n",
    "\n",
    "  #q_aware_model = quantize_model(quant_aware_model)\n",
    "\n",
    "  q_aware_model.compile(optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "  \n",
    "  q_aware_model.summary()\n",
    "\n",
    "  num_batches = tf.data.experimental.cardinality(train_spectrogram_ds).numpy()\n",
    "  print(f\"Number of batches in the training dataset: {num_batches}\")\n",
    "\n",
    "  # finetune quant aware model with quant aware training\n",
    "  q_aware_model.fit(train_spectrogram_ds.take(num_batches // 4), batch_size = 100, epochs=1, validation_data=val_spectrogram_ds)\n",
    "\n",
    "\n",
    "\n",
    "  _, baseline_model_accuracy = model.evaluate(test_spectrogram_ds, verbose=0)\n",
    "\n",
    "  _, q_aware_model_accuracy = q_aware_model.evaluate(test_spectrogram_ds, verbose=0)\n",
    "\n",
    "  print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "  print('Quant test accuracy:', q_aware_model_accuracy)\n",
    "\n",
    "  if save:\n",
    "    # now we haveint8 weights and uint8 activations.\n",
    "    # Convert the model to TFLite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    quantized_tflite_model = converter.convert()\n",
    "    if pruned:\n",
    "      model_filename = f'pruned_qat_{model.name}.tflite'\n",
    "    else:\n",
    "      model_filename = f'qat_{model.name}.tflite'\n",
    "    # Save the quantized model\n",
    "    with open(model_filename, 'wb') as f:\n",
    "      f.write(quantized_tflite_model)\n",
    "\n",
    "    return model_filename\n",
    "  \n",
    "  return q_aware_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the TFLite quantized model\n",
    "def evaluate_tflite_model(interpreter, dataset):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for spectrograms, labels in dataset:\n",
    "        input_data = spectrograms.numpy().astype('float32')\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predicted_labels = tf.argmax(predictions, axis=1)\n",
    "        correct_predictions += tf.reduce_sum(tf.cast(predicted_labels == labels, tf.int32)).numpy()\n",
    "        total_samples += labels.shape[0]\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_qat_models = []\n",
    "pruning_quantized_qat_models = []\n",
    "\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    qat_tflite_model = qat(model, pruning = False, save=True)\n",
    "    quantized_qat_models.append(qat_tflite_model)\n",
    "\n",
    "for model in basic_pruned_models:\n",
    "    pruned_qat_tflite_model = qat(model, pruning = True, save = True)\n",
    "    pruning_quantized_qat_models.append(pruned_qat_tflite_model)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Evaluate the TFLite quantized model\n",
    "interpreter = tf.lite.Interpreter(model_content=qat_models[0])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "\n",
    "#quantized_model_accuracy = evaluate_tflite_model(interpreter, test_spectrogram_ds)\n",
    "#print('Quantized TFLite model test accuracy:', quantized_model_accuracy)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # It returns the size of the gzipped model in kilobytes.\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)/1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in quantized_qat_models:\n",
    "  print(f\"Model: {file}\")\n",
    "  print(f\"Size: {get_gzipped_model_size(file)} KB\")\n",
    "\n",
    "for file in pruning_quantized_qat_models:\n",
    "  print(f\"Model: {file}\")\n",
    "  print(f\"Size: {get_gzipped_model_size(file)} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see persistence of accuracy from tf to tflite\n",
    "def eval_model(interpreter):\n",
    "  test_images, test_labels = ds_to_numpy(test_spectrogram_ds)\n",
    "  \n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print(f\"Evaluated on {i} results so far.\")\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = np.mean((prediction_digits == test_labels))\n",
    "  return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqat_model_file = \"C:\\\\Users\\\\adamk\\\\Downloads\\\\micro_speech_model_making\\\\notebooks\\\\pruned_qat_tiny_embed_conv_model.tflite\"\n",
    "interpreter = tf.lite.Interpreter(pqat_model_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "pqat_test_accuracy = eval_model(interpreter)\n",
    "\n",
    "print('Pruned and quantized TFLite test_accuracy:', pqat_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post training quantization\n",
    "\n",
    "https://ai.google.dev/edge/litert/models/post_training_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to quantize the models using TensorFlow Lite Micro with different granularities\n",
    "def quantize_model(model, optimization_strategy):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    if optimization_strategy == 'weight':\n",
    "        # Quantize only weights\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    elif optimization_strategy == 'full':\n",
    "        # Full integer quantization\n",
    "        def representative_dataset():\n",
    "            for spectrogram, _ in train_spectrogram_ds.take(1000):\n",
    "                yield [spectrogram]\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.uint8\n",
    "        converter.inference_output_type = tf.uint8\n",
    "    elif optimization_strategy == 'dynamic':\n",
    "        # Dynamic range quantization\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimization strategy. Use 'weight', 'full', or 'dynamic'.\")\n",
    "    tflite_model = converter.convert()\n",
    "    return tflite_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to quantize the models using post-training integer quantization with int16 activations and int8 weights\n",
    "def quantize_model_int16x8(model):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    def representative_dataset():\n",
    "        for spectrogram, _ in train_spectrogram_ds.take(100):\n",
    "            yield [spectrogram]\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\n",
    "    converter.inference_input_type = tf.int16\n",
    "    converter.inference_output_type = tf.int16\n",
    "    tflite_model = converter.convert()\n",
    "    return tflite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize each trained model with different granularities and save them\n",
    "quantization_strategies = ['weight', 'full', 'dynamic']\n",
    "\n",
    "for i, model in enumerate(basic_pruned_models):\n",
    "    for strategy in quantization_strategies:\n",
    "        print(f'Quantizing Model {i} with {strategy} strategy')\n",
    "        tflite_model = quantize_model(model, strategy)\n",
    "        model_filename = f'model_{i}_{strategy}_quantized.tflite'\n",
    "        with open(model_filename, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f'Model {i} quantized with {strategy} strategy and saved as {model_filename}')\n",
    "\n",
    "    # Quantize using post-training integer quantization with int16 activations and int8 weights\n",
    "    print(f'Quantizing Model {i} with int16 activations and int8 weights')\n",
    "    tflite_model_int16x8 = quantize_model_int16x8(model)\n",
    "    model_filename_int16x8 = f'model_{i}_int16x8_quantized.tflite'\n",
    "    with open(model_filename_int16x8, 'wb') as f:\n",
    "        f.write(tflite_model_int16x8)\n",
    "    print(f'Model {i} quantized with int16 activations and int8 weights and saved as {model_filename_int16x8}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the quantized models\n",
    "def evaluate_quantized_model(tflite_model_path):\n",
    "    # Load the TFLite model and allocate tensors\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Determine the expected input type\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "\n",
    "\n",
    "    # Evaluate the model on the test dataset\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for spectrogram_batch, label_batch in test_spectrogram_ds:\n",
    "        for spectrogram, label in zip(spectrogram_batch, label_batch):\n",
    "            interpreter.set_tensor(input_details[0]['index'], np.expand_dims(spectrogram, axis=0).astype(input_dtype))\n",
    "            interpreter.invoke()\n",
    "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "            predicted_label = np.argmax(output_data)\n",
    "            total_correct += (predicted_label == label.numpy())\n",
    "            total_samples += 1\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each quantized model\n",
    "for i in range(0, len(models)):\n",
    "    for strategy in quantization_strategies:\n",
    "        model_filename = f'model_{i}_{strategy}_quantized.tflite'\n",
    "        print(f'Evaluating Model {i} with {strategy} quantization')\n",
    "        accuracy = evaluate_quantized_model(model_filename)\n",
    "        print(f'Model {i} with {strategy} quantization accuracy: {accuracy:.4f}')\n",
    "\n",
    "    model_filename_int16x8 = f'model_{i}_int16x8_quantized.tflite'\n",
    "    print(f'Evaluating Model {i} with int16 activations and int8 weights quantization')\n",
    "    accuracy = evaluate_quantized_model(model_filename_int16x8)\n",
    "    print(f'Model {i} with int16 activations and int8 weights quantization accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the size and memory requirements of the quantized models\n",
    "def print_quantized_model_sizes(model_filenames):\n",
    "    sizes = []\n",
    "    for model_filename in model_filenames:\n",
    "        if os.path.exists(model_filename):\n",
    "            model_size = os.path.getsize(model_filename) / 1024  # Size in KB\n",
    "            sizes.append((model_filename, model_size))\n",
    "            print(f'Model: {model_filename} Size: {model_size:.2f} KB')\n",
    "        else:\n",
    "            print(f'Model: {model_filename} not found.')\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of quantized model filenames\n",
    "quantized_model_filenames = []\n",
    "for i in range(0, len(models)):\n",
    "    for strategy in ['weight', 'full', 'dynamic']:\n",
    "        quantized_model_filenames.append(f'model_{i}_{strategy}_quantized.tflite')\n",
    "    quantized_model_filenames.append(f'model_{i}_int16x8_quantized.tflite')\n",
    "\n",
    "# Print sizes of quantized models and store the sizes\n",
    "print(\"\\nQuantized Model Sizes:\")\n",
    "quantized_model_sizes = print_quantized_model_sizes(quantized_model_filenames)\n",
    "\n",
    "# Evaluate each quantized model and collect accuracies\n",
    "quantized_accuracies = []\n",
    "\n",
    "for model_filename, model_size in quantized_model_sizes:\n",
    "    print(f'Evaluating {model_filename}')\n",
    "    accuracy = evaluate_quantized_model(model_filename)\n",
    "    quantized_accuracies.append((model_filename, model_size, accuracy))\n",
    "\n",
    "# Filter accuracies for models smaller than 1000 KB and accuracy greater than 0.8\n",
    "filtered_accuracies = [tup for tup in quantized_accuracies if tup[2] > 0.8 and tup[1] < 1000]\n",
    "\n",
    "# Plot accuracy vs. model size for quantized models\n",
    "model_names, model_sizes, accuracies = zip(*filtered_accuracies)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot points without connecting lines by setting linestyle='None'\n",
    "plt.plot(model_sizes, accuracies, marker='o', linestyle='None', color='r')\n",
    "\n",
    "# Annotate each point with the model name\n",
    "for i, model_name in enumerate(model_names):\n",
    "    plt.text(model_sizes[i], accuracies[i], model_name, fontsize=8, ha='right')\n",
    "\n",
    "plt.xlabel('Model Size (KB)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Model Size for Quantized Models (Models < 1000 KB)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tflite_model(tflite_model_path, test_data, test_labels):\n",
    "    # Load the TFLite model and allocate tensors\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Determine input data type\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "    \n",
    "    # Run inference on the test data\n",
    "    correct_predictions = 0\n",
    "    for i, test_sample in enumerate(test_data):\n",
    "        # Prepare input data, converting to expected dtype if necessary\n",
    "        input_data = np.expand_dims(test_sample, axis=0)\n",
    "        if input_dtype == np.uint8:\n",
    "            input_data = (input_data * 255).astype(np.uint8)  # Scale to UINT8\n",
    "        else:\n",
    "            input_data = input_data.astype(input_dtype)\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        \n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        # Get the output and check the prediction\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predicted_label = np.argmax(output_data)\n",
    "        true_label = test_labels[i]\n",
    "        \n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / len(test_data)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def plot_tflite_size_vs_accuracy(folder_path, test_spectrogram_ds):\n",
    "    test_data, test_labels = ds_to_numpy(test_spectrogram_ds)\n",
    "    # Lists to store results\n",
    "    model_sizes = []\n",
    "    accuracies = []\n",
    "    model_names = []\n",
    "    \n",
    "    # Iterate through all TFLite files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".tflite\"):\n",
    "            model_path = os.path.join(folder_path, filename)\n",
    "            model_name = os.path.splitext(filename)[0]\n",
    "            \n",
    "            # Get model size in KB\n",
    "            model_size = os.path.getsize(model_path) / 1000  # Convert bytes to KB\n",
    "            \n",
    "            # Evaluate the TFLite model accuracy\n",
    "            accuracy = evaluate_tflite_model(model_path, test_data, test_labels)\n",
    "            \n",
    "            # Store results\n",
    "            model_sizes.append(model_size)\n",
    "            accuracies.append(accuracy)\n",
    "            model_names.append(model_name)\n",
    "            \n",
    "            # Print the details\n",
    "            print(f\"Model: {model_name}, Size: {model_size:.2f} KB, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Plotting accuracy vs. model size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(model_sizes, accuracies, marker='o', linestyle='', color='b')\n",
    "\n",
    "    # Annotate each point with the model name\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        plt.text(model_sizes[i], accuracies[i], model_name, fontsize=9, ha='right')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Model Size (KB)')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title('TFLite Model Accuracy vs. Model Size')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return model_sizes, accuracies, model_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sizes, accuracies, model_names = [], [], []\n",
    "folder_path = \"C:\\\\Users\\\\adamk\\\\Downloads\\\\micro_speech_model_making\\\\notebooks\\\\\"\n",
    "model_sizes, accuracies, model_names = plot_tflite_size_vs_accuracy(folder_path, test_spectrogram_ds)\n",
    "# C:\\Users\\adamk\\Downloads\\micro_speech_model_making\\notebooks\\val_spec_ds.tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size, acc, name in zip(model_sizes, accuracies, model_names):\n",
    "    if size < 45 and acc > 0.8:\n",
    "        print(f\"Name : {name}, Size: {size}, Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filtered_model_size_vs_accuracy(model_sizes, accuracies, model_names, max_size, min_accuracy):\n",
    "    # Filter models based on the provided criteria\n",
    "    filtered_sizes = [size for size, acc in zip(model_sizes, accuracies) if size < max_size and acc > min_accuracy]\n",
    "    filtered_accuracies = [acc for size, acc in zip(model_sizes, accuracies) if size < max_size and acc > min_accuracy]\n",
    "    filtered_names = [name for size, acc, name in zip(model_sizes, accuracies, model_names) if size < max_size and acc > min_accuracy]\n",
    "\n",
    "    # Print details of filtered models\n",
    "    for size, acc, name in zip(filtered_sizes, filtered_accuracies, filtered_names):\n",
    "        print(f\"Name: {name}, Size: {size:.2f} KB, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Plot only the filtered models\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(filtered_sizes, filtered_accuracies, marker='o', linestyle='', color='b', label=f\"Size < {max_size} KB & Accuracy > {min_accuracy*100}%\")\n",
    "    \n",
    "    # Annotate each filtered point with the model name\n",
    "    for i, model_name in enumerate(filtered_names):\n",
    "        plt.text(filtered_sizes[i], filtered_accuracies[i], model_name, fontsize=9, ha='right')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Model Size (KB)')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title(f'TFLite Model Accuracy vs. Model Size (Filtered by Size < {max_size} KB & Accuracy > {min_accuracy*100}%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filtered_model_size_vs_accuracy(model_sizes, accuracies, model_names, max_size=50, min_accuracy=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
